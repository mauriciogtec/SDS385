---
title: "SDS 385 Stats Models for Big Data"
subtitle: 'Solutions 01b: Mauricio Garcia Tec'
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE)
```

## Generalized Linear Models

<strong><em>
As an archetypal chase of a GLM, we'll consider the binomial logistic regression model
$$
y_i \sim \mathrm{Binom}(m_i, w_i),
$$
where $y_i$ is an integer number of "successes", $m_i$ is the number of trials for the $i$-tj case, and the succes probability $w_i$ is a regression on a feature vector $x_i$ given by the inverse logit transform
$$
w_i = \frac{1}{1 + \exp\{-x_i^\top\beta\}}.
$$
We want to estimate $\beta$ by the principle of maximum likelihood. For our "binary logistic regression", $m_i = 1$ and $y_i\in\{0.1\}$.

As an aside, if you have a favorite data set or problem that involves a different GLM--say, a Poisson regression for count data--then feel free to work with that model instead throughout this entire section. The fact that we’re working with a logistic regression isn’t essential here; any GLM will do.

#### (A) The Likelihood

Start by writing out the negative log likelihood
$$
l(\beta) = -log\left\{\prod_{i=1}^N p(y_i \mid \beta) \right\}
$$
Simplify your expression as much as possible. This is the thing we want to minimize to compute the MLE. (By longstanding convention, we phrase optimization problems as minimization problems.) 

Derive the gradient of this expression, $\nabla l(\beta)$. Note: your gradient will be a sum of terms $l_i(\beta)$, and it’s OK to use the shorthand
$$
w_i(\beta) = \frac{1}{1 + \exp\{-x_i^\top\beta\}}.
$$
in your expression.
</strong></em>

The model is for the binary logistic regression is
$$
y_i \sim \mathrm{Bernoulli}\left(w_i(\beta)\right)
$$
Or equivalently, that
$$
\mathbb{P}(y_i = x \mid \beta) = \begin{cases} 
  w_i(\beta) & \text{if} & x = 1 \\
  1 - w_i(\beta) & \text{if} & x = 0 \\
  0   & & \text{otherwise} 
\end{cases}
$$
or more succintly
$$
p_i(y_i) = w_i(\beta)^{y_i}( 1 - w_i(\beta))^{1 - y_i} \quad \text{ for } \quad y_i\in\{0,1\}
$$
The loglikelihood loss function is thus
$$
\begin{aligned}
l(\beta) & = - \log \left\{ \prod_{i = 1}^n p_i(y_i \mid \beta ) \right\} \\
& = - \sum_{i=1}^N \log \left\{ p_i(y_i \mid \beta ) \right\} \\
& = - \sum_{i=1}^N \left\{ y_i \log(w_i(\beta)) + (1-y_i)\log(1 - w_i(\beta)) \right\} \\
& = - \sum_{i=1}^N \left\{ \log(1 - w_i(\beta)) + y_i\log\left( \frac{w_i(\beta)}{1 - w_i(\beta)}\right) \right\}. 
\end{aligned}
$$
For the last expression we only used standard properties of the logarithm; the interesting part, however, is that the function $y \mapsto log(y  / (1-y))$ is actually the inverse of $u \mapsto 1/(1 + \exp(-u))$, so the loglikelihood loss function reduces to
$$
\begin{aligned}
l(\beta) & = - \sum_{i=1}^N \left\{ \log(1 - w_i(\beta)) + y_ix_i^\top \beta \right\} \\
 & = - y^\top X \beta - \overline{1}_N^\top \log(1 - w(\beta)),
\end{aligned}
$$
where $\overline{1}_N$ is the vector of all ones. Computing the gradient of $l$ is now easy, we first need to differentiate $w_i(\beta)$
$$
\begin{aligned}
\nabla_\beta w_i(\beta) &= \nabla_\beta \left\{ \left(1 + \exp\{-x_i^\top\beta\} \right)^{-1}  \right\} \\
&= -\left(1 + \exp\{ - x_i^\top\beta\} \right)^{-2} \exp(-x_i^\top \beta)(-x_i) \\
&= -w_i(\beta)^2 \left( \frac{1}{w_i(\beta)} - 1 \right)(-x_i) \\
&=   w_i(\beta)\left(1 - w_i(\beta)\right) x_i.
\end{aligned}
$$
The expression is useful to compute $\nabla_\beta l(\beta)$, but later it will be useful to rewrite the above in a convenient way. The *Jacobian* matrix $\nabla_\beta w(\beta) \in \mathbb{R}^{N \times P}$ of the map $\beta  \mapsto (w_1(\beta),...,w_N(\beta))$ is the matrix whose $i$-th row is $\nabla_iw(\beta)$. Let $W = \textrm{diag}(w(\beta))$, we see that
$$
\nabla_\beta w(\beta) = W(1-W)X.
$$

We will use this result later, for now, we return to derivate $l$ using the chain rule with matrix differentiation (see  `Solution01a` for the recap on matrix differentiation). We now compute $\nabla_\beta l$
$$
\begin{aligned}
\nabla_\beta l(\beta) &= \nabla_\beta  \left\{ - y^\top X \beta - \overline{1}_N^\top \log(1 - w(\beta)) \right\} \\
&= -X^\top y  -  \sum_{i=1}^N \nabla_\beta  \log(1 - w_i(\beta))  \\
&= -X^\top y  +  \sum_{i=1}^N \frac{1}{1-w_i(\beta)}w_i(\beta)(1 - w_i(\beta))x_i  \\
&= -X^\top y  +  \sum_{i=1}^N w_i(\beta)x_i  \\
&=  -X^\top y + X^\top w(\beta)  \\
&=  X^\top \left(w(\beta) - y\right)  \\
\end{aligned}
$$


<strong><em>

#### (B) Solution with Gradient Descent

Read up on the method of steepest descent, i.e. gradient descent, in Nocedal and Wright (see course website). Write your own function that will ﬁt a logistic regression model by gradient descent. Grab the data “wdbc.csv” from the course website, or obtain some other real data that interests you, and test it out. The WDBC ﬁle has information on 569 breast-cancer patients from a study done in Wisconsin. The ﬁrst column is a patient ID, the second column is a classiﬁcation of a breast cell (Malignant or Benign), and the next 30 columns are measurements computed from a digitized image of the cell nucleus. These are things like radius, smoothness, etc. For this problem, use the ﬁrst 10 features for X, i.e. columns 3-12 of the ﬁle. If you use all 30 features you’ll run into trouble.
</strong></em>

For this solution I will assume familiarity with the Gradient Descent Method with Backtracking search. 

##### The Data

The varible $X$ will store 10 features and $y$ the outcome, where $y = 1$ if the cancer is Maligne and $y=0$ otherwise.

```{r, warning=FALSE, message=FALSE}
# Hint: read_csv of tidyverse's package readr is similar read.csv but "cleaner" and somewhat more efficient
library(tidyverse)
wdbc <- read_csv(
  file = url("https://raw.githubusercontent.com/jgscott/SDS385/master/data/wdbc.csv"),
  col_names = FALSE
)
X <- cbind(1, wdbc[ ,3:4]) # 1 adds an intercept
X <- as.matrix(X) # for numerical algorithms
y <- as.integer(wdbc[[2]] == "M")
```

```{r}
head(X)
```
We can check the prevalence of maligne cancer
```{r}
print(table(y))
print(sprintf("The prevalence of maligne cancer is %.2f%%", 100 * sum(y) / length(y)))
```

<!-- We will now write a pair of functions that compute the loglikelihood loss function and the gradient. I will write the functions using the Rcpp framework, which gives a C++ library of R-like classes and seamless R integration to call these functions. More information about the framework [here](http://dirk.eddelbuettel.com/code/rcpp.html). The last time I checked, over 800 CRAN packages used this awesome and super fast framework. For thus uninitiated in the workd of high-performance computing, C++ is one of the fastest programming languages, in particular, it is a *compiled* language, contrary R. The compiler does a nice translation to computer language at compiling time, at does not waste time when we run a command. -->

<!-- ```{r, message=FALSE, warning=FALSE} -->
<!-- library(Rcpp) -->
<!-- ``` -->

<!-- ```{r, engine = 'Rcpp'} -->
<!-- #include <Rcpp.h> -->
<!-- using namespace Rcpp; -->

<!-- // Step 1: Define vectorized (w_i) and Logit Functions -->

<!-- NumericVector logit(NumericVector u) { -->
<!--   int N = u.size(); -->
<!--   NumericVector res(N); -->
<!--   for (int i = 0; i < N; i++) { -->
<!--     res[i] = log(u[i] / (1 - u[i])); -->
<!--   } -->
<!--   return res; -->
<!-- } -->

<!-- NumericVector sigmoid(NumericVector u) { -->
<!--   int N = u.size(); -->
<!--   NumericVector res(N); -->
<!--   for (int i = 0; i < N; i++) { -->
<!--     res[i] = 1 / (1 + exp(-u[i])); -->
<!--   } -->
<!--   return res; -->
<!-- } -->

<!-- // Step 2: Compute LogLoss and Its Gradient -->
<!-- // The Directive [[Rcpp::export]] before the functions tells Rcpp to export the function to R's working environment -->

<!-- // [[Rcpp::export]] -->
<!-- double logloss(NumericVector beta, NumericVector y, NumericMatrix X) { -->
<!--   int N = y.size(); -->
<!--   int P = beta.size(); -->
<!--   double ll = 0; -->
<!--   for (int i = 0; i < N; i++) { -->
<!--     res[i] = 1 / (1 + exp(-u[i])); -->
<!--   }   -->
<!--   return l; -->
<!-- } -->
<!-- ``` -->

##### The Likelihood


The versions of the loglikelihood loss and its gradient we gave are very easy to program in an efficient vectorized way
```{r}

sigmoid <- function(u) {
  # In:
  #   u: vector of N x 1
  # Out: the inverse-logit/sigmoid function of the entries of u
  1 / (1 + exp(-u))
}

logloss <- function(beta, X, y) {
  # In: 
  #   beta: vector of P x 1 of generalized regression coefficients   
  #   X: design MATRIX of N x P
  #   y: vector of N x 1 with binary outcomes
  # Out: the loglikelihood loss
  reg <- X %*% beta # X is usually a dataframe
  w <- sigmoid(as.numeric(reg))
  # output
  as.numeric(-sum(log(1 - w)) - crossprod(y, reg))
}

logloss_grad <- function(beta, X, y) {
  # In: 
  #   beta: vector of P x 1 of generalized regression coefficients   
  #   X: design MATRIX of N x P
  #   y: vector of N x 1 with binary outcomes
  # Out: the gradient of the logloss  
  reg <- X %*% beta # X is usually a dataframe
  w <- sigmoid(as.numeric(reg))
  # output
  as.numeric(crossprod(X, w - y))
}
```

##### Gradient Descent

Now the routines for the Gradient Descent. An important fact: it does not work at all without line search, since the function we want to optimize is very bad behaved. This in fact very well known of the likelihood function.

```{r}
bt_linesearch <- function(x, search_direction, f, grad, control = NULL) {
  # Backtracking Line Search # -------------------------
  # 1. Armijo Rule
  # 2. Curvature Rule
  # Note: This line serach can be applied to many optimization algorithms that find decrease directions
  # Reference: (Nocedal & Wright 41)
  # In: 
  #   x: current point
  #   search_direction: direction where to move
  #   f: function to evaluate
  #   grad: gradient of f (knowing grad(x) would be enough, but this is better presentation)
  #   control:
  #     init_step_size: initial step size in the search_direction, defaults to 1
  #     bt_rate: constant indicating the speed of contraction of the direction, defaults to 0.9
  #     min_decrease_rate: constant indicating the speed of contraction of the direction, defaults to 0.5
  # Out: a constant indicating how many times should a direction be contracted to move to that point
  #      using backtracking linesearch, with Armijo Rule only (no curvature condition)
  #
  # Initialize parameters (defaults/user)
  bt_rate <- 0.9
  min_decrease_rate <- 0.5
  init_step_size <- 1.0
  maxit <- 100
  for (i in seq_along(names(control))) { # override defaults if user
    assign(names(control)[i], control[[i]]) 
  }
  # Store useful constant values
  val <- f(x)
  gradval <- grad(x)
  graddir_prod <- crossprod(gradval, search_direction)
  # Main loop
  step_size <- init_step_size
  repeat {
    newx <- x + step_size * search_direction
    newvalue <- f(newx) 
    min_decrease <- - min_decrease_rate * step_size * graddir_prod
    if (i > maxit || newvalue <= val - min_decrease) {
      break
    } else {
      step_size <- step_size * bt_rate
      i <- i + 1
    }
  }
  if (i == maxit) {
    warning("max number of iterations reached in backtracking")
  }
  # Output
  list(
    newx = newx,
    newvalue = newvalue,
    step_size = step_size,
    iter = i
  )
}
```

```{r}
  
gradient_descent  <- function(x0, f, grad, control=NULL, bt_control=NULL) {
  # In:
  #   x0: initial value
  #   f: function to minimize
  #   grad: gradient of objective function
  #   control: a list with the options for the algorithm
  #     maxit: max number of iterations, defaults 1e4
  #     tol: absolute tolerance for solution convergence, defaults 1e-11
  #     init_step_size: initial step size for the gradient descent, defautls 0.5
  #     recycle_step_size: use lineserach output as initial step_size for next linesearch, defaults false
  #   bt_control: arguments to pass to the control argument of the bt_linesearch function
  #   
  # Out: the a local minimizer of f found using gradient descent with backtracking linesearch
  
  # Default and user control values
  maxit <- 1e4
  tol <- 1e-11
  init_step_size <- 0.5
  recycle_step_size = FALSE
  for (i in seq_along(names(control))) { 
    assign(names(control)[i], control[[i]]) # overwrite default
  }
  # Algorithm
  x <- x0
  oldvalue <- f(x0)
  iter <- 1
  step_size <- init_step_size
  # -- main iteration
  repeat {
    bt_output <- bt_linesearch(
      x = x, 
      search_direction = -grad(x), 
      f = f, 
      grad = grad, 
      control = c(bt_control, list(init_step_size = step_size))
    )
    x <- bt_output$newx
    newvalue <- bt_output$newvalue
    if (recycle_step_size) {
      step_size <- bt_output$step_size  
    }
    # print(bt_output$iter)
    # print(newvalue)
    if (iter >= maxit || newvalue < Inf && abs(oldvalue - newvalue) < tol) {
      break
    } else {
      oldvalue <- newvalue
      iter <- iter + 1
    }
  }
  convergence <- !(iter == maxit)
  # output
  list(
    x = x,
    val = newvalue,
    iter = iter,
    convergence = convergence
  )
}
```

##### The Results

I am gonna obtain our fitted values. Just to make sure I get the right answer, I will check with the **R function** `glm`.

```{r, cache=FALSE}
mod <- glm.fit(X, y, family = binomial())
print(mod$coefficients) 
prediction <- round(mod$fitted.values) 
print(table(y, prediction))
accuracy <- sum(y == prediction) / length(y)
print(sprintf("The (rough) accuracy of the model is %.2f%%", 100 * accuracy))
```

Now **our solution** with our methods. It will be very close, although the optimization problem is very complex for the gradient descent method alone.

```{r}
logit_gdfit <- function(X, y, ...) {
  # In: 
  #   X: design MATRIX of N x P
  #   y: vector of N x 1 with binary outcomes
  #   ...: control and bt_control arguments to pass to the gradient_descent engine.
  # Output:
  #   coefficients: the regression coefficients
  #   fitted_values: the estimated probabilities of incidence
  #   covergence: was there convergence?
  #   iter: iterations of the back engine
  mod <- gradient_descent(
    x0 = numeric(ncol(X)), # vector of zeros
    f = function(beta) logloss(beta, X, y), 
    grad = function(beta) logloss_grad(beta, X, y),
    ...
  )
  # Output
  list(
    coefficients = mod$x,
    fitted_values = as.numeric(sigmoid(X %*% mod$x)),
    convergence = mod$convergence,
    iter = mod$iter
  )
}
```

```{r, cache=TRUE}
control <- list(
  maxit = 1e6,
  tol = 1e-5,
  init_step_size = 1e-4 # I just know, but there's no good reason...
) 
algotime <- system.time({
  ourmod <- logit_gdfit(X, y, control = control)
})
print(algotime)
print(sprintf("%d iterations with convergence=%s", ourmod$iter, ourmod$convergence))
print(ourmod$coefficients) # not too far
prediction <- round(ourmod$fitted_values) 
print(table(y, prediction))
accuracy <- sum(y == prediction) / length(y)
print(sprintf("The (rough) accuracy of the model is %.2f%%", 100 * accuracy))
comparison <- data.frame(
  ourmod_logloss = logloss(ourmod$coefficients, X, y),
  glm_logloss = logloss(mod$coefficients, X, y)
)
print(comparison)
```
With 89.98% we weren't too far from the 93.15% accuracy; we can actually decrease our `tol` and achieve that, but it already took 21 minutes in my fast i7 processor machine already, so we better propose something smarter (even if we had programmed it using C, we will priviledge intelligence over power, for this time). Here's how our achieve logloss compares to the glmfit solution.


#### (B) Newthon's Method

To do a second order optimizatino method we need the Hessian matrix $\nabla^2_\beta l(\beta)$. Fortunately, it is easy to compute, since
$$
\begin{aligned}
\nabla^2_\beta l(\beta) &= \nabla_\beta \left( \nabla_\beta l(\beta) \right) \\
&= \nabla_\beta \left( X^\top(w(\beta) - y)  \right) \\
&= \nabla_\beta \left( X^\top w(\beta) \right) \\
&= X^\top W (1-W) X \\
&= X^\top \tilde{W} X,
\end{aligned}
$$
where we denoted $\tilde{W}=W(1-W)$ to simplify notation. We can now do a quadratic approximation of $l$ near any point $\beta_0$ using the second order taylor polynomial
$$
\begin{aligned}
l(\beta) & = l(\beta_0) + (\beta - \beta_0)^\top \nabla_\beta l(\beta_0) + \frac{1}{2}(\beta - \beta_0)^\top \nabla_\beta^2l(\beta_0)(\beta - \beta_0)  + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
& = l(\beta_0) + (\beta - \beta_0)^\top X^\top (w(\beta_0) - y) + \frac{1}{2}(\beta - \beta_0)^\top X^\top \tilde{W} X (\beta - \beta_0)   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
& = l(\beta_0) + (X\beta - X\beta_0)^\top (w(\beta_0) - y) + \frac{1}{2}(X\beta - X\beta_0)^\top  \tilde{W}  (X\beta - X\beta_0)   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
& = l(\beta_0) + h^\top (w(\beta_0) - y) + \frac{1}{2}h^\top  \tilde{W} h   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
\end{aligned}
$$
We can further simplify this expression by completing the square in the quatratic expression above, taking $h$ as the variable, yielding
$$
\begin{aligned}
& l(\beta_0) + h^\top (w(\beta_0) - y) + \frac{1}{2}h^\top  \tilde{W} h   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
&=
\frac{1}{2} \left[h - \tilde{W}^{-1}(w(\beta_0) - y)\right]^\top\tilde{W}\left[h - \tilde{W}^{-1}(w(\beta_0) - y) \right] + \text{const} + O\left(\lVert \beta - \beta_0 \rVert^3 \right),
\end{aligned}
$$
where $\text{const}$, is a generic constant not depending on $h$ (or $\beta$). Writing again in terms of $\beta$, we get
$$
l(\beta) = \frac{1}{2}\left[X\beta - z\right]^\top\tilde{W}\left[X\beta - z\right] + \text{const.} +  O\left(\lVert \beta - \beta_0 \rVert^3 \right)
$$
where $z = X\beta_0 + \tilde{W}^{-1}(w(\beta_0) - y)$. It's interesting because we can think of $z$ as the sum of the current regression value plus a weighted error in the approximation given by 
$$
\tilde{W}^{-1}(w(\beta_0) - y) = \sum_i (w_i(\beta_0) - y) / (w_i(\beta)(1-w_i(\beta)) = \alpha^\top \epsilon 
$$
for working errors $\epsilon_i = w_i(\beta_0) - y_i$, weighted by $\alpha_i = (w_i(\beta)(1-w_i(\beta)))^{-1}$.
Newthon's method 
