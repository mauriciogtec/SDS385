---
title: "SDS 385 Stats Models for Big Data"
subtitle: 'Solutions 01b: Mauricio Garcia Tec'
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

## Generalized Linear Models

<strong><em>
As an archetypal chase of a GLM, we'll consider the binomial logistic regression model
$$
y_i \sim \mathrm{Binom}(m_i, w_i),
$$
where $y_i$ is an integer number of "successes", $m_i$ is the number of trials for the $i$-tj case, and the succes probability $w_i$ is a regression on a feature vector $x_i$ given by the inverse logit transform 
$$
w_i = \frac{1}{1 + \exp\{-x_i^\top\beta\}}.
$$
We want to estimate $\beta$ by the principle of maximum likelihood. For our "binary logistic regression", $m_i = 1$ and $y_i\in\{0.1\}$.

As an aside, if you have a favorite data set or problem that involves a different GLM--say, a Poisson regression for count data--then feel free to work with that model instead throughout this entire section. The fact that we’re working with a logistic regression isn’t essential here; any GLM will do.

#### (A) The Likelihood

Start by writing out the negative log likelihood
$$
l(\beta) = -\log\left\{\prod_{i=1}^N p(y_i \mid \beta) \right\}
$$
Simplify your expression as much as possible. This is the thing we want to minimize to compute the MLE. (By longstanding convention, we phrase optimization problems as minimization problems.) 

Derive the gradient of this expression, $\nabla l(\beta)$. Note: your gradient will be a sum of terms $l_i(\beta)$, and it’s OK to use the shorthand
$$
w_i(\beta) = \frac{1}{1 + \exp\{-x_i^\top\beta\}}.
$$
in your expression.
</strong></em>

Our model is a binomial logistic regression is
$$
y_i | x_i, \beta \sim \mathrm{Binom}\left(m_i, w_i(\beta)\right)
$$
Or equivalently, 
$$
\mathbb{P}(y_i = y \mid \beta) = \binom{m_i}{y}w_i(\beta)^y(1 - w_i(\beta))^{m_i - y}
$$
The loglikelihood loss function is thus
$$
\begin{aligned}
l(\beta) & = - \log \left\{ \prod_{i = 1}^n p_i(y_i \mid \beta ) \right\} \\
& = - \sum_{i=1}^N \log \left\{ p_i(y_i \mid \beta ) \right\} \\
& = - \sum_{i=1}^N \left\{ y_i \log(w_i(\beta)) + (m_i-y_i)\log(1 - w_i(\beta)) \right\} \\
\end{aligned}
$$
This expression is very comfortable because it is easily vectorized in a computer language. 

Just as we did in the previous exercise with linear regression, in order to optimize this logloss function we must find $\hat{\beta}$ satisfying the first order conditions $\nabla l(\hat{\beta}) = 0$. Since the gradient depends on the derivatives of the $w_i(\beta)$, we first need to obtain their gradients.

Define the sigmoid function
$$
\begin{aligned}
\sigma \colon & \mathbb{R} \to [0,1] \\ 
& u \mapsto \frac{1}{1 + e^{-u}}.
\end{aligned}
$$
It is clear that $w_i(\beta) = \sigma(x_i^\top \beta)$. This function has nice derivative properties
$$
\begin{aligned}
\frac{\partial}{\partial u}\sigma(u)  &= \frac{\partial}{\partial u} \left(1 + \exp(-u) \right)^{-1} \\
&= -\left(1 + \exp(-u) \right)^{-2} \exp(-u) \\
&= -\sigma(u)^2 \left( \frac{1}{\sigma(u)} - 1 \right) \\
&= -\sigma(u)\left(1 - \sigma(u)\right).
\end{aligned}
$$
From the chain rule (and using one of our matrix differentiation formulas of last exercise) it is clear that

$$
\nabla_\beta w_i(\beta) = w_i(\beta)(1 - w_i(\beta))x_i
$$
This again can be improved in matrix form, which we will nade for later exercises. The *Jacobian* matrix $\nabla_\beta w(\beta) \in \mathbb{R}^{N \times P}$ of the map $\beta  \mapsto (w_1(\beta),...,w_N(\beta))$ is the matrix whose $i$-th row is $\nabla_iw(\beta)$. If we write $W = \textrm{diag}(w(\beta))$, we see that
$$
\nabla_\beta w(\beta) = W(1-W)X.
$$

We will use this result later, for now, we return to computing the gradient of $l$ using the chain rule
$$
\begin{aligned}
\nabla_\beta l(\beta) &= \nabla_\beta  \left\{ - \sum_{i = 1}^N y_i\log(w_i(\beta)) + (m_i - y_i)\log(1 - w_i(\beta)) \right\} \\
&= - \sum_{i = 1}^N \left\{ y_i \frac{\nabla_\beta{w_i(\beta)}}{w_i(\beta)} -  (m_i - y_i) \frac{\nabla_\beta{w_i(\beta)}}{ 1- w_i(\beta)} \right\} \\
&= - \sum_{i = 1}^N \left\{ y_i (1 - w_i(\beta))x_i -  (m_i - y_i)w_i(\beta)x_i \right\} \\
&= - \sum_{i = 1}^N  (y_i - m_iw_i(\beta))x_i. \\
\end{aligned}
$$
The last expression is really beautiful, because if $w_i(\beta)$ represents the success probabilit, then $m_iw_i(\beta)$ is the expected number of successes, which we are directly comparing to the true number of successes $y_i$.

Is not hard to verify that the expression for $\nabla_\beta l(\beta)$ above can be written in matrix form, which will result more efficient for the computers
$$
\nabla_\beta l(\beta) = -X^\top(y - W m) = -X^\top(y - w \otimes m),
$$
where $\otimes$ means entrywise multiplication.

<strong><em>

#### (B) Solution with Gradient Descent

Read up on the method of steepest descent, i.e. gradient descent, in Nocedal and Wright (see course website). Write your own function that will ﬁt a logistic regression model by gradient descent. Grab the data “wdbc.csv” from the course website, or obtain some other real data that interests you, and test it out. The WDBC ﬁle has information on 569 breast-cancer patients from a study done in Wisconsin. The ﬁrst column is a patient ID, the second column is a classiﬁcation of a breast cell (Malignant or Benign), and the next 30 columns are measurements computed from a digitized image of the cell nucleus. These are things like radius, smoothness, etc. For this problem, use the ﬁrst 10 features for X, i.e. columns 3-12 of the ﬁle. If you use all 30 features you’ll run into trouble.
</strong></em>

For this exercise I will assume the reader is familiar with gradient descent, which is a technique for solving unrestricted optimization problems of the form
$$
\min_u f(u)
$$
when $f$ is continously differentiable and hopefully convex. At each iteration, the steepest descent method picks a step size $\alpha_i$ (a.k.a. learning rate) and an initial guess $u_0$ and makes updates according to the rule
$$
u_i \mapsto u_{i -1} - \alpha_i \nabla_u f(u_{i-1}).
$$
At a basic level, one starts with an arbitrary fixed $\alpha$ used for all values of $\alpha_i$. Linesearch algorithms make the method much more robust by automatically selecting a good choice of $\alpha$ at every step. The rationale for linesearch is that the gradient always gives a direction where the function can be decreased with very small movements, but not necessarily for large ones; linesearch seeks a step not too small, not too large, with a minimum decrease. I will implement a basic version of backtracking linesearch which is a simple technique for sequentially reducing the step size until reasonable decrease is found.

##### The Data

The varible $X$ will store 10 features and $y$ the outcome, where $y = 1$ if the cancer is Maligne and $y=0$ otherwise. For computation purposes, the features will be scaled, meaning that for each column $x^j$ we will do a transformation
$$
x^j \leftarrow \frac{x^j - \mathrm{mean}(x^j)}{\mathrm{stdev}(x^j)},
$$
this will make all columns centered (zero mean) and scaled (standard deviation one). This helps a lot to improve the performance of algorithms since the resultant likelihood is better behaved. This might not be necessary for more advanced optimization methods, such as Newton's method, but is is necessary for gradient descent.


```{r, warning=FALSE, message=FALSE}
# Hint: read_csv of tidyverse's package readr is similar read.csv but "cleaner" and somewhat more efficient
library(tidyverse)
wdbc <- read_csv(
  file = url("https://raw.githubusercontent.com/jgscott/SDS385/master/data/wdbc.csv"),
  col_names = FALSE
)
X <- wdbc[ ,3:12] %>% 
  scale() %>%
  cbind(1, .) %>% 
  as.matrix()
y <-(wdbc[[2]] == "M") %>% 
  as.integer()
m <- 1
```

```{r}
head(X)
```
We can check the prevalence of maligne cancer
```{r}
print(table(y))
print(sprintf("The prevalence of maligne cancer is %.2f%%", 100 * sum(y) / length(y)))
```

##### The Likelihood

We compute simple versions of the loglikelihood loss and related functions

```{r}
#' @title Sigmoid function
#' @description computes u -> 1 / (1 + exp(-u))
#' @params
#'   u: a numeric vector 
#'   eps: a smoothing parameter to reduce perfect 0's and 1's
#' @value a numeric vector with sigmoid(u_i) in each entry
sigmoid <- function(u, eps = 0) {
  # Input data type validation
  stopifnot(is.numeric(u), is.numeric(eps))
  # Compute sigmoid
  s <-  1 / (1 + exp(-u))
  # Fix perfect 0 and 1
  if (eps > 0) {
    s[s < eps] <- eps
    s[s > 1 - eps] <- 1 - eps
  }
  # Output
  s
}

#' @title Binomial logloss function
#' @description Computes the logloss of a model y | X, beta ~ Binom(m, sigmoid(Xbeta))
#' @params
#'   beta: a numeric vector of coefficients
#'   X: a numeric matrix of regressors
#'   y: an integer vector of observations
#'   m: an integer or integer vector of the max count number for y_i,
#'      it must have eitherlength one or the length of y.
#' @details X can be any matrix from the Matrix package
#' @value a real number representing the negative of the loglikelihood of the model
logloss <- function(beta, X, y, m = 1) {
  # Input data type validation
  stopifnot(
    is.numeric(beta), is.numeric(y), is.numeric(m),
    inherits(X, c("matrix", "Matrix")),
    length(beta) == ncol(X)
  )
  # regression coefficients
  reg <- X %*% beta 
  # predicted probabilities
  w <- sigmoid(drop(reg), eps = 1e-5)
  # output
  -sum(y * log(w) + (m - y) * log(1 - w))
}

#' @title Binomial logloss function gradient
#' @description Computes the gradient of the logloss of a model y | X, beta ~ Binom(m, sigmoid(Xbeta))
#' @params
#'   beta: a numeric vector of coefficients
#'   X: a numeric matrix of regressors
#'   y: an integer vector of observations
#'   m: an integer or integer vector of the max count number for y_i,
#'      it must have eitherlength one or the length of y.
#' @details X can be any matrix from the Matrix package
#' @value a numeric vector with the gradient of the negative of the loglikelihood of the model
logloss_grad <- function(beta, X, y, m = 1) {
  # Input data type validation
  stopifnot(
    is.numeric(beta), is.numeric(y), is.numeric(m),
    inherits(X, c("matrix", "Matrix")),
    length(beta) == ncol(X)
  )
  # regression coefficients
  reg <- X %*% beta 
  w <- sigmoid(drop(reg), eps = 1e-5)
  # output
  -drop(crossprod(X, y - m*w))
}
```

##### Gradient Descent without linesearch

We now program a first version of gradient descent without lineserach to get the idea clear:

```{r}
#' @title Gradient descent without linesearch
#' @description Minimizes an input function f
#' @params
#'   x0: a numeric vector of initial guess
#'   f: the function to mimize
#'   grad: the gradient of f
#'   control: a list with the parameters of the gradient descent algorithm (see details)
#' @details The control can receive the following parameters, default values are shown
#'   maxit: 10000, max number of iterations
#'   tol: 1e-4, tolerance for relative convergence
#'   history: TRUE, if TRUE saves all values of f during the iterations
#'   step: 1 / sqrt(nrow(X)), learning rate
#' @value a list with 
#'   optimizer: numeric vector that is a critical point of f
#'   value: min value reachd
#'   converged: flag indicating convergence
#'   iter: number of iterations
#'   history: empty if history is FALSE, otherwise the value of f at every iteration
gradient_descent  <- function(x0, f, grad, control=NULL) {
  stopifnot(
    is.numeric(x0), is.function(f), is.function(grad)
  )
  # Assign default control values and override for user input
  maxit <- 1e4
  tol <- 1e-4
  step <- 1 / nrow(X)
  history <-  TRUE
  fhist <- numeric(0)
  for (i in seq_along(names(control))) { 
    assign(names(control)[i], control[[i]]) # overwrite default
  }
  # Initial values
  x <- x0
  fold <- f(x)
  # Main loop
  iter <- 1
  converged <- FALSE
  repeat {
    # save values in fhist if history is asked
    if (history) {
      fhist <- c(fhist, fold)
    }
    search_direction = -grad(x)
    # Move along the negative gradient
    x <- x + step * search_direction
    fnew <- f(x)
    # Check convergence
    converged <- abs((fnew - fold) / fold) < tol
    # Exit if convergence
    if (converged) {
      break
    }
    # Exit if max number of iterations reached
    if (iter >= maxit) {
      warning("max number of iterations reached") 
      break
    } 
    iter <- iter + 1
    fold  <- fnew
  }
  # Output
  list(
    optimizer = x,
    value = fnew,
    converged = converged,
    iter = iter,
    history = fhist
  )
}
```

##### Fitting with gradient descent

```{r}
#' @title Binomial regression with Gradient Descent
#' @description Fits the MLE model y | X, beta ~ Binom(m, sigmoid(Xbeta))
#' @params
#'   X: a numeric matrix of regressors
#'   y: an integer vector of observations
#'   m: an integer or integer vector of the max count number for y_i,
#'      it must have eitherlength one or the length of y.
#'   gd_control: a list of parameters to be passed to the gradient descent optimizer
#' @details X can be any matrix from the Matrix package
#' @value a list with solution of the model including:
#'   coefficients: the regression coefficients
#'   fitted_values: the estimated probabilities of incidence
#'   coverged: was there convergence?
#'   iter: iterations of the back engine
#'   history: if history=TRUE is passed in gd_control, it return the value of logloss at each iteration
binom_gdfit <- function(X, y, m = 1, gd_control = NULL) {
  # Input data type validation
  stopifnot(
    is.numeric(y), is.numeric(m),
    inherits(X, c("matrix", "Matrix"))
  )
  # Define the optimization functions and initial value
  f <- function(beta) logloss(beta, X, y, m)
  grad <- function(beta) logloss_grad(beta, X, y, m)
  beta0 <- numeric(ncol(X))
  # Call Gradient Descent
  sol <- gradient_descent(beta0, f, grad, gd_control)
  # Output
  list(
    coefficients = sol$optimizer,
    fitted_values = sigmoid(drop(X %*% sol$optimizer)),
    converged = sol$converged,
    iter = sol$iter,
    history = sol$history
  )
}
```


```{r}
control <- list(
  history = TRUE,
  tol = 1e-4
)
algotime <- system.time({
  ourmod <- binom_gdfit(X, y, gd_control = control)
})
print(algotime)
print(sprintf("%d iterations with convergence=%s", ourmod$iter, ourmod$converged))
print(ourmod$coefficients) # not too far
prediction <- round(ourmod$fitted_values)
print(table(y, prediction))
accuracy <- sum(y == prediction) / length(y)
print(sprintf("The (rough) accuracy of the model is %.2f%%", 100 * accuracy))
```

We had an awesome accuracy rate of 94.20% in very little time. Although we did have to use a low tolerance value for convergence since the problem is not very well behaved. We plot the convergence of the loglikelihood.

```{r}
plot(
 ourmod$history, 
 xlab = "iteration", 
 ylab = "logloss", 
 type = "l", 
 col = "blue",
 main = "Convergence of negative loglikelihood with Gradient Descent"
)
```

We now test our model and compare it to the `glm` function in R.


```{r}
mod <- glm.fit(X, y, family = binomial())
print(mod$coefficients)
prediction <- round(mod$fitted.values)
print(table(y, prediction))
accuracy <- sum(y == prediction) / length(y)
print(sprintf("The (rough) accuracy of the model is %.2f%%", 100 * accuracy))
```

Doing a comparison, we see that our optimization result was fairly good.

```{r}
comparison <- data.frame(
  ourmod_logloss = logloss(ourmod$coefficients, X, y),
  glm_logloss = logloss(mod$coefficients, X, y)
)
print(comparison)
```

#### (Bonus section) Including linesearch

The cool thing about backtracking linesearch is that it is super easy to implement, it works not only for gradient descent, but any search direction method, and it will require a very small change to are gradient descent to incorporate.

Let's define a function that fines a good step

```{r}
#' @title Linesearch with Backtraking
#' @description Find a goos tep decreasing an objective function. It reduces the step with a factor (step <- bt_rate*step) until a minimum decrease is min_decrease_rate * step * grad'*search_direction is found
#' @params
#'   x: a numeric vector of the current point of iteration
#'   f: a function to mimize
#'   grad: gradient of the function f
#'   search_direction: a numeric vector representing a search direction
#'   control: a list with the parameters for lineserach, defaults are
#'     bt_rate: 0.9 speed at which to decrease step size in search
#'     min_decrease_rate: 0.1 see Nocedal & Wright for details
#'     init_step: 1 first step size to try
#'     maxit: 100 max number of iterations attempted during linesearch
#' @value A list with
#'   xnew: the new point after moving
#'   fnew: value of f at new point
#'   step: good step size satisfying the Armijo condition
#'   iter: number of backtracking iterations
#'   success: indicating whether a decrease was found
bt_linesearch <- function(x, search_direction, f, grad, control = NULL) {
  # Input data type validation
  stopifnot(
    is.numeric(x), is.numeric(search_direction),
    length(x) == length(search_direction),
    is.function(f), is.function(grad)
  )
  # Default controls and override with user values
  bt_rate <- 0.9
  min_decrease_rate <- 0.1
  init_step <- 1
  maxit <- 1e3
  for (i in seq_along(names(control))) { # override defaults if user
    assign(names(control)[i], control[[i]])
  }
  # Store useful constant values
  fold <- f(x)
  gradval <- grad(x)
  graddir_prod <- crossprod(gradval, search_direction)
  # Main loop
  i  <- 1
  step <- init_step
  success = FALSE
  repeat {
    xnew <- x + step * search_direction
    fnew <- f(xnew)
    min_decrease <-  max(-min_decrease_rate * step * graddir_prod,0)
    if (fnew < fold - min_decrease) {
      success = TRUE
      break
    }
    if (i == maxit) {
      warning("max number of iterations reached")
      break
    } 
    step <- step * bt_rate
    i <- i + 1
  }
  # Output
  list(
    xnew = xnew,
    fnew = fnew,
    step = step,
    iter = i,
    success = success
  )
}
```

Now let's do the very small modification to our previous gradient descent function. The bonimial fitting function will only change in that it will include an arguments for lineserach controls.

#### (B) Newton's Method

##### Newton's minimization algorithm

For simplicity of presentation, I will assumme that $m_i = 1$ for the rest of this section.

**Second order approximation** To do a second order optimization method we need the Hessian matrix $\nabla^2_\beta l(\beta)$. Fortunately, it is easy to compute, since
$$
\begin{aligned}
\nabla^2_\beta l(\beta) &= \nabla_\beta \left( \nabla_\beta l(\beta) \right) \\
&= - \nabla_\beta \left( X^\top(y - w(\beta))  \right) \\
&= \nabla_\beta \left( X^\top w(\beta) \right) \\
&=  X^\top W (1-W) X \\
&= X^\top \tilde{W} X,
\end{aligned}
$$
where we denoted $\tilde{W}=W(1-W)$ to simplify notation. We can now do a quadratic approximation of $l$ near any point $\beta_0$ using the second order taylor polynomial
$$
\begin{aligned}
l(\beta) & = l(\beta_0) + (\beta - \beta_0)^\top \nabla_\beta l(\beta_0) + \frac{1}{2}(\beta - \beta_0)^\top \nabla_\beta^2l(\beta_0)(\beta - \beta_0)  + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
& = l(\beta_0) + (\beta - \beta_0)^\top X^\top (w(\beta_0) - y) + \frac{1}{2}(\beta - \beta_0)^\top X^\top \tilde{W} X (\beta - \beta_0)   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
& = l(\beta_0) + (X\beta - X\beta_0)^\top (w(\beta_0) - y) + \frac{1}{2}(X\beta - X\beta_0)^\top  \tilde{W}  (X\beta - X\beta_0)   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
& = l(\beta_0) + h^\top (w(\beta_0) - y) + \frac{1}{2}h^\top  \tilde{W} h   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
\end{aligned}
$$
We can further simplify this expression by completing the square in the quatratic expression above, taking $h$ as the variable, yielding
$$
\begin{aligned}
& l(\beta_0) + h^\top (w(\beta_0) - y) + \frac{1}{2}h^\top  \tilde{W} h   + O\left(\lVert \beta - \beta_0 \rVert^3 \right) \\
&=
\frac{1}{2} \left[h - \tilde{W}^{-1}(w(\beta_0) - y)\right]^\top\tilde{W}\left[h - \tilde{W}^{-1}(w(\beta_0) - y) \right] + \text{const} + O\left(\lVert \beta - \beta_0 \rVert^3 \right),
\end{aligned}
$$
where $\text{const}$, is a generic constant not depending on $h$ (or $\beta$). Writing again in terms of $\beta$, we get
$$
l(\beta) = \frac{1}{2}\left[X\beta - z\right]^\top\tilde{W}\left[X\beta - z\right] + \text{const.} +  O\left(\lVert \beta - \beta_0 \rVert^3 \right)
$$
where $z = X\beta_0 + \tilde{W}^{-1}(y - w(\beta_0))$. It's interesting because we can think of $z$ as the sum of the current regression value plus a weighted error in the approximation given by
$$
\tilde{W}^{-1}(w(\beta_0) - y) = \sum_i (w_i(\beta_0) - y) / (w_i(\beta)(1-w_i(\beta)) = \alpha^\top \epsilon
$$
for working errors $\epsilon_i = w_i(\beta_0) - y_i$, weighted by $\alpha_i = (w_i(\beta)(1-w_i(\beta)))^{-1}$.

**The algorithm**. Newton's method for the minimum of a function $f\in C^2$ consists of approximation $f$ at a canditate solution $x_i$, using the second order taylor expansion
$$
f(x) \approx f(x_i) + \nabla f(x_i)^\top (x-x_i) + \frac{1}{2}(x - x_i)^\top\nabla^2 f(x_i)(x-x_i),
$$
we then finding the minimum of this approximation, and take resulting minimizer $x_{i+1}$ as the next candidate solution. We see that the taylor approximation has an exact solution
$$
x_{i + 1} = x_i - (\nabla^2 f)^{-1}\nabla f(x_i).
$$
It can be proved that under our assumptions of smoothness,  there exists a radius near every local minimizer $x^*$, in which the of convergence $x_i \to x^*$ is guaranteed, and has a quadratic rate.

**Slight improvement**. We recognize that $-(\nabla^2 f)^{-1}\nabla f(x_i)$ is a direction in which we are moving to find an optimal solution. Could we take shorter or longer steps in this direction? The answer is: Newton's method has a natural step/learning rate of one, since the quadratic converge is guaranteed once one is sufficiently closed to an optimizer. However, one could try to do smaller or larger steps at the beginning, to make sure the function is actually decreasing enough. The strategies to a find a step size are the same as in the gradient descent case.

**Summary of the method for logistic regression**. In the beginning of this section we noticed that the problem of finding the new $x_{i+1}$ is equivalent to solving a weighted least square problem of the form
$$
l(\beta) = \frac{1}{2}\left(X\beta - z\right)^\top\tilde{W}\left(X\beta - z\right).
$$
On the other hand, computing
$$
- (\nabla^2 f)^{-1}(x_i)\nabla f(x_i)
$$
is not desirable, since it involvings inverting a matrix, specifically, the normal equations matrix $X^\top \tilde{W}X$. We can instead solve the linear regression system efficiently using QR or other numerically stable technique. That is, solve the WLS problem
$$
\hat{\beta}_{i + 1} = \mathrm{argmin}_\beta\; \left(z_i - X\beta\right)^\top\tilde{W}_i\left(z_i - X\beta\right)
$$
where $\tilde{W}_i$ and $z$ depend on $\hat{\beta}_i$.

##### Implementation of Newton's method in R

We could implement Newton's method in one simple function, but to be better protected to errors, we use linesearch too, so we first estimate the Newton direction and then mode in that direction with linesearch.

```{r}
logit_newton_direction <- function(beta0, X, y) {
  # Input type validation
  stopifnot(
    inherits(X, c("Matrix", "matrix")),
    is.numeric(y)
  )
  # Computations to obtain the working weights and responses for intermediate WLS
  reg <- drop(X %*% beta0) # Regression at beta0
  w <- sigmoid(reg, eps = 1e-10) # Vector of current predictions
  error <- y - w  # prediction error
  wlm_weights <- w * (1 - w)
  wlm_responses <- reg + error / wlm_weights # working responses
  # Perform weighted least squares to obtain new point
  betanew <- lm.wfit(X, wlm_responses, wlm_weights)$coefficients
  # New direction is the vector difference
  newton_direction <- betanew - beta0
  # Output
  newton_direction
}
```


```{r}
logit_newton_fit <- function(X, y,
                             history = TRUE,
                             control = NULL,
                             linesearch = TRUE,
                             ls_control = NULL) {
  # Input type validation
  stopifnot(
    inherits(X, c("Matrix", "matrix")),
    is.numeric(y)
  )
  # Default controls and user values
  maxit <- 1e4
  tol <- 1e-4
  history <-  TRUE
  for (i in seq_along(names(control))) { 
    assign(names(control)[i], control[[i]]) # overwrite default
  }
  # Define function to be optimized
  f <- function(beta) logloss(beta, X, y)
  grad <-  function(beta) logloss_grad(beta, X, y) # necessary for linesearch only
  # Initial values
  beta <- numeric(ncol(X))
  fold <- f(beta)
  step <- 1
  # Historical measures (not necessary for real implementation)
  logloss_hist = fold
  step_hist = numeric()
  accuracy_hist <-  0.5
  # Main loop
  iter <- 1
  converged <- FALSE
  repeat {
    # Search direction and lineserach
    newton_direction = logit_newton_direction(beta, X, y)
    if (linesearch) {
      step <- bt_linesearch(beta, newton_direction, f, grad, ls_control)$step
    }
    # Move along the negative gradient
    beta <- beta + step * newton_direction
    fnew <- f(beta)
    # Check convergence
    converged <- abs((fnew - fold) / fold) < tol
    # Exit if convergence
    if (converged) {
      break
    }
    # Exit if max number of iterations reached
    if (iter >= maxit) {
      warning("max number of iterations reached") 
      break
    } 
    if (history) {
      predicted <- round(drop(sigmoid(X %*% beta)))
      logloss_hist <- c(logloss_hist, fnew)
      step_hist = c(step_hist, step)
      accuracy_hist= c(accuracy_hist, sum(predicted == y) / length(y))
    }
    iter <- iter + 1
    fold  <- fnew
  }
  # Some output measures
  fitted_values <- drop(sigmoid(X %*% beta))
  accuracy <- sum(round(fitted_values) == y) / length(y)
  # Output
  list(
    coefficients = beta,
    fitted_values = fitted_values,
    accuracy = accuracy,
    loglikelihood = -fnew,
    iter = iter,
    converged = converged,
    logloss_hist = logloss_hist,
    step_hist = step_hist,
    accuracy_hist = accuracy_hist
  )
}
```

```{r}
control <- list(
  tol = 1e-11
)
algotime <- system.time({
  mod <- logit_newton_fit(X, y, control = control, history = TRUE)
})
print(algotime)
print(paste("The algorithm converge in only", mod$iter, "iterations!"))
print(mod$coefficients)
print(mod$loglikelihood)
print(paste("The model reached an accuracy of", scales::percent(mod$accuracy), "in prediction!"))
```

In the following plot, we will see that--as expected--the step size remained of size one. Newton has a natural step size of one within its convergence region.

```{r, fig.width=12}
# Test
par(mfrow = c(2,3))
plot(mod$fitted_values, y, xlab = "fitted", ylab = "y", main = "fitted vs predicted", pch = 21, cex = 2, bg = scales::alpha("blue", 0.15))
hist(y - mod$fitted_values, xlab = "residuals", main = "residuals", col = scales::alpha("green", 0.2))
plot(mod$fitted_values, y - mod$fitted_values, xlab = "fitted", ylab = "residuals", main = "residuals vs fited", cex = 2, pch = 21, bg = scales::alpha("red", 0.15))
plot(mod$logloss_hist, type = "l", col = "blue", xlab = "iter", main = "logloss value")
plot(mod$step_hist, type = "l", col = "red",  xlab = "iter", main = "step size")
plot(mod$accuracy_hist, type = "l", col = "green",  xlab = "iter", main = "accuracy per iteration")
```

